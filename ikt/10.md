<div align="center" style="margin-top: 16px;">
    <strong>OtÃ¡zka 10</strong>
</div>

<nav style="
    position: sticky;
    top: 0;
    z-index: 100;
    background: rgba(0,0,0,0.8);
    padding: 8px 0 4px 0;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    text-align: center;
">
    <a href="10.md" style="color:white; text-decoration:none; margin: 0 16px;">â¬…ï¸ PÅ™edchozÃ­</a>
    <a href="../README.md" style="color:white; text-decoration:none; margin: 0 16px;">ğŸ  DomÅ¯</a>
    <a href="11.md" style="color:white; text-decoration:none; margin: 0 16px;">NÃ¡sledujÃ­cÃ­ â¡ï¸</a>
</nav>

## [Materialy z pÅ™edmÄ›tu ZUI](https://carnation-airport-4a0.notion.site/ZUI-ZKOU-KA-172361e34903809dbc38dcdcf12096b8)

---

# UmÄ›lÃ¡ inteligence a neuronovÃ© sÃ­tÄ›

**OtÃ¡zka:** Å˜eÅ¡enÃ­ problÃ©mÅ¯ v oblasti zÃ¡vÄ›reÄnÃ© prÃ¡ce pomocÃ­ metod umÄ›lÃ© inteligence (teorie hranÃ­ her, prohledÃ¡vÃ¡nÃ­ stavovÃ©ho prostoru, evoluÄnÃ­ algoritmy, strojovÃ© uÄenÃ­ aj.), aspekty nasazenÃ­ tÄ›chto metod (vÃ½hody, omezenÃ­); umÄ›lÃ© neuronovÃ© sÃ­tÄ› a jejich pouÅ¾itelnost v kontextu zÃ¡vÄ›reÄnÃ© prÃ¡ce, typickÃ© Ãºlohy nasazenÃ­, typy neuronovÃ½ch sÃ­tÃ­, matematickÃ½ popis modelu, algoritmy a strategie uÄenÃ­, zhodnocenÃ­ uÄÃ­cÃ­ch algoritmÅ¯

---

> **PoznÃ¡mka:** UmÄ›lÃ¡ inteligence je sloÅ¾itÃ© tÃ©ma, takÅ¾e tento soubor je o nÄ›co obsÃ¡hlejÅ¡Ã­ neÅ¾ ostatnÃ­. VÄ›tÅ¡ina tÃ©mat takÃ© obsahuje video na YouTube pro ty, kteÅ™Ã­ preferujÃ­ vizuÃ¡lnÃ­ zpÅ¯sob uÄenÃ­.

## 1. Definice umÄ›lÃ© inteligence
UmÄ›lÃ¡ inteligence je schopnost poÄÃ­taÄovÃ½ch systÃ©mÅ¯ vykonÃ¡vat Ãºkoly, kterÃ© normÃ¡lnÄ› vyÅ¾adujÃ­ lidskou inteligenci, napÅ™Ã­klad:

- **RozpoznÃ¡vÃ¡nÃ­ Å™eÄi:** Schopnost AI systÃ©mÅ¯ pÅ™evÃ¡dÄ›t mluvenou Å™eÄ do textu. (google asistent)
- **VizuÃ¡lnÃ­ vnÃ­mÃ¡nÃ­:** Schopnost AI identifikovat a interpretovat vizuÃ¡lnÃ­ informace z okolnÃ­ho prostÅ™edÃ­ (rozpoznÃ¡vÃ¡nÃ­ obliÄejÅ¯).
- **RozhodovÃ¡nÃ­:** Schopnost AI analyzovat data a dÄ›lat rozhodnutÃ­ na zÃ¡kladÄ› analÃ½zy dat. (DoporuÄenÃ­ produktÅ¯ podle nÃ¡kupnÃ­ historie)



### 1.1. Historie a vÃ½voj AI
Historie AI sahÃ¡ do poloviny 20. stoletÃ­, kdy byla poprvÃ© pÅ™edstavena myÅ¡lenka strojÅ¯ schopnÃ½ch vykonÃ¡vat lidskÃ© Ãºkoly. PrvnÃ­mi prÅ¯kopnÃ­ky byli Alan Turing a John McCarthy. V prÅ¯bÄ›hu let se AI vyvinula od jednoduchÃ½ch logickÃ½ch systÃ©mÅ¯ k pokroÄilÃ½m neuronovÃ½m sÃ­tÃ­m a strojovÃ©mu uÄenÃ­ jak je zÃ¡nme dnes.

## 2. Teorie hranÃ­ her
**(Wikipedie)** Teorie her je studium matematickÃ½ch modelÅ¯ strategickÃ½ch interakcÃ­ mezi racionÃ¡lnÃ­mi agenty s aplikacemi v ekonomii, logice, systÃ©movÃ½ch vÄ›dÃ¡ch a informatice. PÅ¯vodnÄ› se zamÄ›Å™ovala na dvouosobÃ© hry s nulovÃ½m souÄtem, kde zisk jednoho ÃºÄastnÃ­ka je vyvÃ¡Å¾en ztrÃ¡tou druhÃ©ho, ale v 50. letech 20. stoletÃ­ se rozÅ¡Ã­Å™ila na hry s nenulovÃ½m souÄtem a rÅ¯znÃ© behaviorÃ¡lnÃ­ vztahy. ModernÃ­ teorie her zaÄala s dÅ¯kazem smÃ­Å¡enÃ½ch strategiÃ­ rovnovÃ¡hy Johna von Neumanna, kterÃ½ pouÅ¾il Brouwerovu vÄ›tu o pevnÃ©m bodÄ›, coÅ¾ vedlo k vlivnÃ© knize "Theory of Games and Economic Behavior" s Oskarem Morgensternem. Teorie her byla rozsÃ¡hle rozvinuta v 50. letech a aplikovÃ¡na na evoluci v 70. letech a nynÃ­ je klÃ­ÄovÃ½m nÃ¡strojem v mnoha oborech, s Å™adou nositelÅ¯ Nobelovy ceny mezi jejÃ­mi pÅ™ispÄ›vateli.

**(JÃ¡)** Teorie her je studium toho, jak lidÃ© dÄ›lajÃ­ rozhodnutÃ­ v situacÃ­ch, kde kaÅ¾dÃ¡ volba ovlivÅˆujÄ› ostatnÃ­. PomÃ¡hÃ¡ urÄit nejlepÅ¡Ã­ strategie v adversÃ¡nÃ­ch i spolupracujÃ­cÃ­ch scÃ©nÃ¡Å™Ã­ch, jako je ekonomie, politika nebo IT. ZjednoduÅ¡enÄ› Å™eÄeno, pÅ™edpovÃ­dÃ¡, co lidÃ© udÄ›lajÃ­ ve hrÃ¡ch, vyjednÃ¡vÃ¡nÃ­ch a konfliktech.

### 2.1. Teorie her + AI
V AI se teorie her pouÅ¾Ã­vÃ¡ k vÃ½voji algoritmÅ¯, kterÃ© mohou Å™eÅ¡it konfliktnÃ­ situace a optimalizovat rozhodovÃ¡nÃ­. 

PÅ™Ã­klady zahrnujÃ­ vyjednÃ¡vacÃ­ systÃ©my nebo aukce.

### 2.2. PÅ™Ã­klady algoritmÅ¯ (Minimax, Alpha-beta oÅ™ezÃ¡vÃ¡nÃ­)
- **Minimax:** Tento algoritmus se pouÅ¾Ã­vÃ¡ v rozhodovacÃ­ch procesech, kde se stÅ™Ã­dajÃ­ dva hrÃ¡Äi. CÃ­lem je minimalizovat maximÃ¡lnÃ­ moÅ¾nou ztrÃ¡tu. (Doslova je to backtracking z rekurzivnÃ­ch algoritmÅ¯)

  <img src="img/10/ai_minimax.gif" width="500px" />
  <img src="img/10/minimax.png" width="500px" />

- **Alpha-beta proÅ™ezÃ¡vÃ¡nÃ­:** Optimalizace Minimax algoritmu, kterÃ¡ zkracuje vÄ›tve ve stromu moÅ¾nostÃ­, kterÃ© nemohou ovlivnit koneÄnÃ© rozhodnutÃ­ (jsou redundantnÃ­), coÅ¾ zvyÅ¡uje efektivitu vÃ½poÄtu.

  <img src="img/10/ai_alphabeta.gif" width="500px" />

[Alpha beta](https://tutorialforbeginner.com/alpha-beta-pruning-in-ai#what-is-alpha-beta-pruning)

---

ObÄ› jsme dÄ›lali na cvikÃ¡ch takÅ¾e poÄÃ­tÃ¡m Å¾e se na to asi zeptaj. [Super video na vysvÄ›tlenÃ­](https://www.youtube.com/watch?v=l-hh51ncgDI).


## 3. ProhledÃ¡vÃ¡nÃ­ stavovÃ©ho prostoru
### 3.1. Co je stavovÃ½ prostor
StavovÃ½ prostor je abstraktnÃ­ koncept, kterÃ½ pÅ™edstavuje vÅ¡echny moÅ¾nÃ© stavy, do kterÃ½ch se mÅ¯Å¾e systÃ©m dostat, a vÅ¡echny moÅ¾nÃ© cesty/pÅ™echody mezi tÄ›mito stavy. 

<img src="img/10/state_space.png" width="500px" />

### 3.2. Metody prohledÃ¡vÃ¡nÃ­

- **BFS (Breadth-First Search)**: Metoda prohledÃ¡vÃ¡nÃ­ do Å¡Ã­Å™ky. ProhledÃ¡vÃ¡ vÅ¡echny moÅ¾nÃ© stavy na jednÃ© Ãºrovni pÅ™ed tÃ­m, neÅ¾ pÅ™ejde na dalÅ¡Ã­ ÃºroveÅˆ. Je uÅ¾iteÄnÃ¡ pro nalezenÃ­ nejkratÅ¡Ã­ cesty v neohodnocenÃ©m grafu = harny nemajÃ­ pÅ™iÅ™azenÃ© Å¾Ã¡dnÃ© vÃ¡hy nebo hodnoty. [Super video na vysvÄ›tlenÃ­](https://www.youtube.com/watch?v=HZ5YTanv5QE).
  
- **DFS (Depth-First Search)**: Metoda prohledÃ¡vÃ¡nÃ­ do hloubky. ProhledÃ¡vÃ¡ co nejhloubÄ›ji v grafu, neÅ¾ se vrÃ¡tÃ­ zpÄ›t a pokraÄuje na dalÅ¡Ã­ vÄ›tvi. Je efektivnÃ­ pro prohledÃ¡vÃ¡nÃ­ velkÃ½ch stavovÃ½ch prostorÅ¯, ale nemusÃ­ najÃ­t nejkratÅ¡Ã­ cestu. [Super video na vysvÄ›tlenÃ­](https://www.youtube.com/watch?v=Urx87-NMm6c).
  
  <img src="img/10/dfs_bfs.gif" width="500px" />
  
- **A***: HeuristickÃ¡ metoda prohledÃ¡vÃ¡nÃ­, kterÃ¡ kombinuje vlastnosti BFS a DFS a pouÅ¾Ã­vÃ¡ odhadovacÃ­ funkci k nalezenÃ­ nejefektivnÄ›jÅ¡Ã­ cesty. Je velmi efektivnÃ­ pro hledÃ¡nÃ­ optimÃ¡lnÃ­ cesty v ohodnocenÃ½ch grafech. 
  **POLOPATISMUS**: Algoritmus vÃ­ kterÃ½m smÄ›rem je cÃ­l a preferuje nody, kterÃ© jsou blÃ­Å¾ k cÃ­li. Proto na gifu jde vidÄ›t Å¾e algoritmus jede znaÄnÄ› rychleji smÄ›rem k cÃ­li.
  [Super video na vysvÄ›tlenÃ­](https://www.youtube.com/watch?v=71CEj4gKDnE)

  <img src="img/10/astar.gif" width="500px" />

### 3.3. PraktickÃ© aplikace a pÅ™Ã­klady

- **Robotika**: Roboti pouÅ¾Ã­vajÃ­ tyto algoritmy k navigaci ve svÃ©m prostÅ™edÃ­ a k nalezenÃ­ optimÃ¡lnÃ­ch tras k dosaÅ¾enÃ­ cÃ­lÅ¯.
  
- **Hry**: HledÃ¡nÃ­ optimÃ¡lnÃ­ch tahÅ¯ ve hrÃ¡ch, jako jsou Å¡achy nebo puzzle hry, kde algoritmy jako Minimax a A* pomÃ¡hajÃ­ simulovat moÅ¾nÃ© tahy a jejich dÅ¯sledky. (Pokud si to chcete zkusit [tady je dost cool cviÄenÃ­ na implementaci N-Queens problÃ©mu za pouÅ¾itÃ­ backtrackingu. Jen pro nadÅ¡ence.](https://leetcode.com/problems/n-queens/description/))
  
- **Logistika**: Optimalizace trasy pro doruÄovacÃ­ sluÅ¾by (viz A* algoritmus na mapÄ›), kde je tÅ™eba najÃ­t nejefektivnÄ›jÅ¡Ã­ zpÅ¯sob doruÄenÃ­ zÃ¡silek.

## 4. EvoluÄnÃ­ algoritmy

### 4.1. ZÃ¡kladnÃ­ principy evoluÄnÃ­ch algoritmÅ¯
EvoluÄnÃ­ algoritmy (EA) jsou inspirovÃ¡ny pÅ™Ã­rodnÃ­mi evoluÄnÃ­mi procesy a jsou pouÅ¾Ã­vÃ¡ny k Å™eÅ¡enÃ­ optimalizaÄnÃ­ch a hledacÃ­ch problÃ©mÅ¯.

  <img src="img/10/evolve.png" width="500px" />

KlÃ­ÄovÃ© principy:

- **Populace a generace**: Algoritmy pracujÃ­ s populacÃ­ jedincÅ¯, kterÃ¡ se v prÅ¯bÄ›hu generacÃ­ vyvÃ­jÃ­.
- **SelektivnÃ­ tlak**: Jedinci s vyÅ¡Å¡Ã­ kvalitou (fitness) majÃ­ vyÅ¡Å¡Ã­ pravdÄ›podobnost vÃ½bÄ›ru pro reprodukci.
- **KÅ™Ã­Å¾enÃ­ (Crossover)**: Kombinace genetickÃ©ho materiÃ¡lu dvou rodiÄÅ¯ k vytvoÅ™enÃ­ novÃ©ho potomka.
- **Mutace**: NÃ¡hodnÃ© zmÄ›ny v genetickÃ©m materiÃ¡lu jedince pro udrÅ¾enÃ­ genetickÃ© rozmanitosti.
- **Fitness funkce**: HodnocenÃ­ kvality jednotlivÃ½ch jedincÅ¯ v populaci, kterÃ© urÄuje jejich pravdÄ›podobnost pÅ™eÅ¾itÃ­ a reprodukce.

### 4.2. GenetickÃ© algoritmy
GenetickÃ© algoritmy (GA) jsou typem EA. ZÃ¡kladnÃ­ kroky genetickÃ©ho algoritmu zahrnujÃ­:

1. **Inicializace**: VytvoÅ™enÃ­ poÄÃ¡teÄnÃ­ populace nÃ¡hodnÃ½ch jedincÅ¯.
2. **HodnocenÃ­**: VÃ½poÄet fitness hodnoty kaÅ¾dÃ©ho jedince v populaci.
3. **VÃ½bÄ›r**: VÃ½bÄ›r rodiÄÅ¯ na zÃ¡kladÄ› jejich fitness hodnoty (napÅ™. ruletovÃ½m kolem nebo turnajovÃ½m vÃ½bÄ›rem).
4. **KÅ™Ã­Å¾enÃ­ (Crossover)**: Kombinace genetickÃ½ch informacÃ­ rodiÄÅ¯ k vytvoÅ™enÃ­ potomkÅ¯.
5. **Mutace**: NÃ¡hodnÃ© Ãºpravy genetickÃ©ho kÃ³du potomkÅ¯.
6. **NahrazenÃ­**: VytvoÅ™enÃ­ novÃ© generace nahrazenÃ­m ÄÃ¡sti nebo celÃ© populace potomky.
7. **Iterace**: OpakovÃ¡nÃ­ krokÅ¯ hodnocenÃ­, vÃ½bÄ›ru, kÅ™Ã­Å¾enÃ­, mutace a nahrazenÃ­, dokud nenÃ­ dosaÅ¾eno ukonÄovacÃ­ho kritÃ©ria (napÅ™. dosaÅ¾enÃ­ urÄitÃ© kvality nebo maximÃ¡lnÃ­ poÄet generacÃ­).
  <img src="img/10/ga.png" width="500px" />

### 4.3. PÅ™Ã­klady pouÅ¾itÃ­ a jejich efektivita

- **Optimalizace**: **NEJDÅ®LEÅ½ITÄšJÅ Ã BOD** NapÅ™Ã­klad optimalizace tvaru letadlovÃ½ch kÅ™Ã­del, kde je cÃ­lem minimalizovat odpor vzduchu pÅ™i zachovÃ¡nÃ­ pevnosti konstrukce. PÅ™Ã­padnÄ› jakÃ½chkoliv jinÃ½ch komponent.
- **StrojovÃ© uÄenÃ­**: TrÃ©novÃ¡nÃ­ modelÅ¯ neuronovÃ½ch sÃ­tÃ­, kde evoluÄnÃ­ algoritmy mohou pomoci najÃ­t optimÃ¡lnÃ­ strukturu sÃ­tÄ› a parametry.
- **Hry a umÄ›lÃ¡ inteligence**: VytvÃ¡Å™enÃ­ strategiÃ­ pro hrÃ¡Äe v komplexnÃ­ch hrÃ¡ch, jako je Å¡achy nebo GO.

PÅ™Ã­klad kostry auta vytvoÅ™enÃ© GA (vÃ½sledek je o polovinu lehÄÃ­ se stejnou robustnostÃ­ neÅ¾ beÅ¾nÃ¡ lisovanÃ¡ kostra):

<img src="img/10/ga_auto.png" width="500px" />

### 4.4. Specifika a pÅ™Ã­klad genetickÃ©ho algoritmu

PÅ™Ã­klad batohu je klasickou optimalizaÄnÃ­ Ãºlohou pro GA:

**ProblÃ©m**: MÃ¡me batoh s omezenou nosnostÃ­ a soubor pÅ™edmÄ›tÅ¯ s rÅ¯znou hmotnostÃ­ a hodnotou. CÃ­lem je vybrat takovou kombinaci pÅ™edmÄ›tÅ¯, jejichÅ¾ celkovÃ¡ hodnota je maximÃ¡lnÃ­ a zÃ¡roveÅˆ nepÅ™esahuje hmotnostnÃ­ limit batohu.

1. **Inicializace**: VytvoÅ™Ã­me poÄÃ¡teÄnÃ­ populaci nÃ¡hodnÃ½ch Å™eÅ¡enÃ­, kde kaÅ¾dÃ© Å™eÅ¡enÃ­ reprezentuje jednu kombinaci pÅ™edmÄ›tÅ¯ v batohu (napÅ™Ã­klad pomocÃ­ binÃ¡rnÃ­ho vektoru, kde 1 znamenÃ¡, Å¾e pÅ™edmÄ›t je zahrnut v batohu a 0 ne).
2. **HodnocenÃ­**: KaÅ¾dÃ© Å™eÅ¡enÃ­ ohodnotÃ­me podle celkovÃ© hodnoty pÅ™edmÄ›tÅ¯ v batohu, pokud splÅˆuje hmotnostnÃ­ limit.
3. **Selekce**: Vybereme nejlepÅ¡Ã­ Å™eÅ¡enÃ­ na zÃ¡kladÄ› jejich fitness hodnot.
4. **KÅ™Ã­Å¾enÃ­ a Mutace**: Aplikujeme kÅ™Ã­Å¾enÃ­ a mutace k vygenerovÃ¡nÃ­ novÃ½ch Å™eÅ¡enÃ­ z vybranÃ½ch.
5. **NahrazenÃ­ a Iterace**: NovÄ› vytvoÅ™enÃ¡ Å™eÅ¡enÃ­ nahradÃ­ nÄ›kterÃ¡ stÃ¡vajÃ­cÃ­ v populaci a proces se opakuje, dokud nenÃ­ dosaÅ¾eno ukonÄovacÃ­ho kritÃ©ria (napÅ™. poÄet generacÃ­ nebo ÄasovÃ½ limit).


**Efektivita**:
- EvoluÄnÃ­ algoritmy jsou efektivnÃ­ pro problÃ©my s velkÃ½m a sloÅ¾itÃ½m vyhledÃ¡vacÃ­m prostorem, kde tradiÄnÃ­ metody Äasto selÅ¾ou nebo jsou vÃ½poÄetnÄ› nÃ¡roÄnÃ©.
- DÃ­ky svÃ© paralelnÃ­ povaze mohou bÃ½t snadno Å¡kÃ¡lovÃ¡ny na vÃ­ce procesorÅ¯ nebo distribuovanÃ© systÃ©my. (PyGAD pro Keras/PyTorch pouÅ¾Ã­vÃ¡ defaultnÄ› GPU pro paralelnÃ­ zpracovÃ¡nÃ­)
- Jsou robustnÃ­ vÅ¯Äi lokÃ¡lnÃ­m extrÃ©mÅ¯m dÃ­ky nÃ¡hodnÃ½m mutacÃ­m a selektivnÃ­mu tlaku.

[Zase jedno krÃ¡tkÃ© video](https://www.youtube.com/watch?v=-kpcAa-qKwY)



## 5. StrojovÃ© uÄenÃ­
UmoÅ¾ÅˆÄ›nÃ­ poÄÃ­taÄovÃ½m systÃ©mÅ¯m se automaticky uÄit a zlepÅ¡ovat ze zkuÅ¡enostÃ­ bez explicitnÃ­ho naprogramovÃ¡nÃ­. StrojovÃ© uÄenÃ­ se dÄ›lÃ­ na nÄ›kolik hlavnÃ­ch typÅ¯:

- **SupervizovanÃ© uÄenÃ­ (s uÄitelem)**: Algoritmy se uÄÃ­ z pÅ™edem oznaÄenÃ½ch datovÃ½ch sad, kde kaÅ¾dÃ¡ vstupnÃ­ data majÃ­ pÅ™iÅ™azenÃ© sprÃ¡vnÃ© vÃ½stupy (labely). CÃ­lem je nauÄit model predikovat vÃ½stupy pro novÃ©, neoznaÄenÃ© vstupy.

  <img src="img/10/supervised.png" width="500px" />
  
- **NesupervizovanÃ© uÄenÃ­ (bez ucitele)**: Algoritmy se uÄÃ­ z neoznaÄenÃ½ch datovÃ½ch sad, kde neexistujÃ­ Å¾Ã¡dnÃ© vÃ½stupnÃ­ labely. CÃ­lem je identifikovat skrytÃ© vzory nebo struktury v datech, kterÃ½ch jsme si my nevÅ¡imli.

  <img src="img/10/unsupervised.png" width="500px" />

- **PolosupervizovanÃ© uÄenÃ­ (asi s polouÄitelem? :D)**: Kombinace supervizovanÃ©ho a nesupervizovanÃ©ho uÄenÃ­, kde se algoritmus uÄÃ­ z malÃ© mnoÅ¾iny oznaÄenÃ½ch dat a velkÃ©ho mnoÅ¾stvÃ­ neoznaÄenÃ½ch dat.
  
- **Reinforcement Learning (UÄenÃ­ posÃ­lenÃ­m)**: Algoritmy se uÄÃ­ skrze interakce s prostÅ™edÃ­m, kde dostÃ¡vajÃ­ zpÄ›tnou vazbu ve formÄ› odmÄ›n nebo trestÅ¯. CÃ­lem je nauÄit se optimÃ¡lnÃ­ strategii jednÃ¡nÃ­. Taky se tomu Å™Ã­kÃ¡ **"cukr a biÄ"**.

  <img src="img/10/reinforcement.png" width="500px" />

### 5.1. SupervizovanÃ© vs. nesupervizovanÃ© uÄenÃ­
- **SupervizovanÃ© uÄenÃ­**:
  - **PÅ™Ã­klady**: Regrese (pÅ™edpovÄ›Ä ÄÃ­selnÃ½ch hodnot), klasifikace (rozdÄ›lenÃ­ dat do kategoriÃ­).
  - **VÃ½hody**: VysokÃ¡ pÅ™esnost pÅ™i dostatku oznaÄenÃ½ch dat, snadnÃ¡ interpretace vÃ½sledkÅ¯.
  - **NevÃ½hody**: PotÅ™eba velkÃ©ho mnoÅ¾stvÃ­ oznaÄenÃ½ch dat, kterÃ¡ mohou bÃ½t nÃ¡kladnÃ¡ na zÃ­skÃ¡nÃ­. (Viz nÃ¡Å¡ pain s label studiem)

- **NesupervizovanÃ© uÄenÃ­**:
  - **PÅ™Ã­klady**: Clustering (seskupovÃ¡nÃ­ podobnÃ½ch datovÃ½ch bodÅ¯), asociace (nalezenÃ­ vzorÅ¯ a pravidel).
  - **VÃ½hody**: NevyÅ¾aduje oznaÄenÃ¡ data, vhodnÃ© pro prÅ¯zkum a objevovÃ¡nÃ­ novÃ½ch vzorÅ¯.
  - **NevÃ½hody**: VÃ½sledky mohou bÃ½t hÅ¯Å™e interpretovatelnÃ©, mÅ¯Å¾e bÃ½t tÄ›Å¾kÃ© zvolit sprÃ¡vnÃ© metriky pro hodnocenÃ­ kvality modelu. Algoritmus mÅ¯Å¾e najÃ­t jinÃ© vztahy, neÅ¾ jsme chtÄ›li.

### 5.2. PÅ™Ã­klady algoritmÅ¯

  - **LineÃ¡rnÃ­ regrese**: Predikuje ÄÃ­selnÃ© hodnoty na zÃ¡kladÄ› lineÃ¡rnÃ­ho vztahu mezi vstupnÃ­mi a vÃ½stupnÃ­mi daty. Jeden z nejjednoduÅ¡Ã­ch zpÅ¯sobÅ¯ predikce. **PÅ™Ã­klad:** Predikce ceny nemovitostÃ­ na zÃ¡kladÄ› velikosti, lokace a dalÅ¡Ã­ch faktorÅ¯. [Video](https://www.youtube.com/watch?v=CtsRRUddV2s)

    <img src="img/10/lin_reg.png" width="500px" />
  
  - **LogistickÃ¡ regrese**: PouÅ¾Ã­vÃ¡ se pro klasifikaci, kde vÃ½stupy jsou klasifikovÃ¡ny do kategoriÃ­ podle kÅ™ivky (logistickÃ¡ funkce). [Video](https://www.youtube.com/watch?v=EKm0spFxFG4)
  
    <img src="img/10/log_reg.png" width="500px" />
  
  - **K-nejbliÅ¾Å¡Ã­ch sousedÅ¯ (K-NN)**: Klasifikuje data na zÃ¡kladÄ› jejich podobnosti s nejbliÅ¾Å¡Ã­mi sousedy v trÃ©novacÃ­ sadÄ›. **PÅ™Ã­klad**: Detekce spamu v e-mailech, rozpoznÃ¡vÃ¡nÃ­ rukopisnÃ½ch ÄÃ­slic. [Video](https://www.youtube.com/watch?v=0p0o5cmgLdE)

    <img src="img/10/knn.png" width="500px" />

PoloÅ¾ky nÃ­Å¾e jsou spÃ­Å¡e navÃ­c, nejsou v studijnÃ­ch materiÃ¡lech ale v literatuÅ™e jsou spolu dost spojovÃ¡ny:
  - **K-means**: Skupinuje data do K klastrÅ¯. Je to podobnÃ© K-NN, ale mÃ­sto nejbliÅ¾Å¡Ã­ho souseda se hledÃ¡ nejbliÅ¾Å¡Ã­ stÅ™ed. StÅ™edy se sami aktualizujÃ­, coÅ¾ algoritmu umoÅ¾Åˆuje adaptovat se dobÅ™e na novÃ¡ data. [Video](https://www.youtube.com/watch?v=R2e3Ls9H_fc)

  - **HierarchickÃ© clustering**: VytvÃ¡Å™Ã­ hierarchii klastrÅ¯ buÄ slouÄenÃ­m menÅ¡Ã­ch klastrÅ¯, nebo rozdÄ›lenÃ­m vÄ›tÅ¡Ã­ch. HodnÄ› pouÅ¾Ã­vanÃ© u heatmap a GA vÃ½sledkÅ¯. [Video](https://www.youtube.com/watch?v=7xHsRkOdVwo)


## 6. Aspekty nasazenÃ­ metod umÄ›lÃ© inteligence

### 6.1. Teorie her
- **VÃ½hody**:
  - PomÃ¡hÃ¡ modelovat a analyzovat strategickÃ© rozhodovÃ¡nÃ­ v konkurenÄnÃ­ch a kooperativnÃ­ch scÃ©nÃ¡Å™Ã­ch.
  - UÅ¾iteÄnÃ¡ v ekonomii, politice a sociÃ¡lnÃ­ch vÄ›dÃ¡ch pro pÅ™edpovÃ­dÃ¡nÃ­ chovÃ¡nÃ­ ÃºÄastnÃ­kÅ¯.
- **NevÃ½hody**:
  - Komplexnost modelÅ¯ mÅ¯Å¾e bÃ½t vysokÃ¡, coÅ¾ mÅ¯Å¾e ztÄ›Å¾ovat jejich praktickÃ© pouÅ¾itÃ­.
  - VyÅ¾aduje pÅ™esnÃ© znalosti o preferencÃ­ch a strategiÃ­ch vÅ¡ech zÃºÄastnÄ›nÃ½ch stran, jinak ztrÃ¡cÃ­ pÅ™esnost.

### 6.2. ProhledÃ¡vÃ¡nÃ­ stavovÃ©ho prostoru
- **VÃ½hody**:
  - EfektivnÃ­ pro nalezenÃ­ optimÃ¡lnÃ­ch Å™eÅ¡enÃ­ v rÅ¯znÃ½ch problÃ©mech, jako jsou puzzle nebo hry (Å¡achy).
  - UmoÅ¾Åˆuje systematickÃ© prozkoumÃ¡nÃ­ vÅ¡ech moÅ¾nÃ½ch stavÅ¯ a cest.
- **NevÃ½hody**:
  - MÅ¯Å¾e bÃ½t vÃ½poÄetnÄ› nÃ¡roÄnÃ©, zvlÃ¡Å¡tÄ› u velkÃ½ch stavovÃ½ch prostorÅ¯. 
  - NÄ›kterÃ© metody mohou bÃ½t neefektivnÃ­ bez sprÃ¡vnÃ©ho heuristickÃ©ho vedenÃ­. (A*)

### 6.3. EvoluÄnÃ­ algoritmy
- **VÃ½hody**:
  - SchopnÃ© nalÃ©zt kvalitnÃ­ Å™eÅ¡enÃ­ pro sloÅ¾itÃ© optimalizaÄnÃ­ problÃ©my.
  - RobustnÃ­ vÅ¯Äi lokÃ¡lnÃ­m minimÅ¯m dÃ­ky nÃ¡hodnÃ½m mutacÃ­m a selektivnÃ­mu tlaku.
- **NevÃ½hody**:
  - VyÅ¾adujÃ­ mnoho iteracÃ­, coÅ¾ mÅ¯Å¾e bÃ½t ÄasovÄ› a vÃ½poÄetnÄ› nÃ¡roÄnÃ© oproti alternativÃ¡m.
  - ÃšspÄ›Å¡nost zÃ¡visÃ­ na sprÃ¡vnÃ©m nastavenÃ­ parametrÅ¯, jako jsou mutaÄnÃ­ rychlost a velikost populace. MÅ¯Å¾e pÅ¯sobit jako Black Box.

### 6.4. StrojovÃ© uÄenÃ­
- **VÃ½hody**:
  - Schopnost automaticky se zlepÅ¡ovat a adaptovat na novÃ¡ data.
  - Å irokÃ© spektrum aplikacÃ­, od predikce a klasifikace po rozpoznÃ¡vÃ¡nÃ­ vzorÅ¯ a rozhodovÃ¡nÃ­.
- **NevÃ½hody**:
  - PotÅ™eba velkÃ©ho mnoÅ¾stvÃ­ kvalitnÃ­ch dat pro trÃ©nink. (Existuje pouÅ¾itÃ­ syntetickÃ½ch dat co toto trochu odlehÄuje)
  - MÅ¯Å¾e bÃ½t obtÃ­Å¾nÃ© interpretovat modely a vÃ½sledky, zejmÃ©na u sloÅ¾itÃ½ch algoritmÅ¯ jako hlubokÃ© uÄenÃ­.


## 7. NeuronovÃ© SÃ­tÄ›

NeuronovÃ© sÃ­tÄ› (NN) jsou inspirovÃ¡ny biologickÃ½mi neurony v lidskÃ©m mozku a pÅ™edstavujÃ­ zÃ¡kladnÃ­ vÃ½poÄetnÃ­ jednotky, kterÃ© pÅ™ijÃ­majÃ­ vstupy, provÃ¡dÃ­ vÃ½poÄty a generujÃ­ vÃ½stupy. Tyto vÃ½stupy mohou bÃ½t pÅ™edÃ¡ny do dalÅ¡Ã­ch neuronÅ¯, coÅ¾ umoÅ¾Åˆuje sÃ­tÃ­m uÄit se.

[NejlepÅ¡Ã­ mega pecka video1](https://www.youtube.com/watch?v=aircAruvnKk&t=984s)

### 7.1. Neuron

Neuron je zÃ¡kladnÃ­ stavebnÃ­ jednotkou neuronovÃ© sÃ­tÄ›, inspirovanou biologickÃ½mi neurony v lidskÃ©m mozku. KaÅ¾dÃ½ neuron pÅ™ijÃ­mÃ¡ vstupy, provÃ¡dÃ­ vÃ½poÄty a generuje vÃ½stup, kterÃ½ mÅ¯Å¾e bÃ½t pÅ™edÃ¡n do dalÅ¡Ã­ch neuronÅ¯. TÃ­mto zpÅ¯sobem neuronovÃ© sÃ­tÄ› mohou modelovat sloÅ¾itÃ© nelineÃ¡rnÃ­ vztahy mezi vstupy a vÃ½stupy.

### 7.2. Struktura Neuronu

Neuron se sklÃ¡dÃ¡ z nÄ›kolika klÃ­ÄovÃ½ch komponent:

- **Vstupy (Inputs)**: KaÅ¾dÃ½ neuron pÅ™ijÃ­mÃ¡ signÃ¡ly z jinÃ½ch neuronÅ¯ nebo vstupnÃ­ch datovÃ½ch bodÅ¯. Tyto signÃ¡ly jsou vÃ¡Å¾enÃ©, coÅ¾ znamenÃ¡, Å¾e kaÅ¾dÃ½ vstup mÃ¡ pÅ™idruÅ¾enou vÃ¡hu, kterÃ¡ urÄuje jeho dÅ¯leÅ¾itost.
- **VÃ¡hy (Weights)**: VÃ¡hy jsou nastavitelnÃ© parametry, kterÃ© se bÄ›hem trÃ©ninku neuronovÃ© sÃ­tÄ› upravujÃ­, aby se minimalizovala chyba (viz uÄenÃ­). VÃ¡hy urÄujÃ­, jak silnÄ› jednotlivÃ© vstupy ovlivÅˆujÃ­ vÃ½stup.
- **Bias**: Bias je dalÅ¡Ã­ parametr, kterÃ½ se pÅ™iÄÃ­tÃ¡ k vÃ¡Å¾enÃ©mu souÄtu vstupÅ¯. PomÃ¡hÃ¡ modelu lÃ©pe se pÅ™izpÅ¯sobit trÃ©ninkovÃ½m datÅ¯m.
- **AktivaÄnÃ­ funkce (Activation Function)**: Po vÃ½poÄtu vÃ¡Å¾enÃ©ho souÄtu vstupÅ¯ a biasu je vÃ½slednÃ¡ hodnota prohnÃ¡na aktivaÄnÃ­ funkcÃ­, kterÃ¡ pÅ™idÃ¡vÃ¡ nelinearitu do modelu a umoÅ¾Åˆuje neuronovÃ© sÃ­ti modelovat sloÅ¾itÃ© vztahy.

<img src="img/10/neuronComparison.png" width="500px" />

### 7.3. VÃ½poÄet v Neuronu

VÃ½stup neuronu \(y\) se vypoÄÃ­tÃ¡ nÃ¡sledujÃ­cÃ­m zpÅ¯sobem:

1. **VÃ¡Å¾enÃ½ souÄet**: Nejprve se spoÄÃ­tÃ¡ vÃ¡Å¾enÃ½ souÄet vstupÅ¯ a biasu.  
   $$z = \sum_{i=1}^{n} (x_i \cdot w_i) + b$$  
   kde \(x_i\) jsou vstupy, \(w_i\) jsou vÃ¡hy a \(b\) je bias.

2. **AktivaÄnÃ­ funkce**: PotÃ© se na vÃ½sledek \(z\) aplikuje aktivaÄnÃ­ funkce \( \sigma(z) \), kterÃ¡ urÄÃ­ finÃ¡lnÃ­ vÃ½stup neuronu.  
   $$y = \sigma(z)$$

### 7.4. Typy aktivaÄnÃ­ch funkcÃ­

   - **Sigmoid**: Produkuje vÃ½stupy v rozmezÃ­ (0, 1), Äasto pouÅ¾Ã­vanÃ¡ v poslednÃ­ vrstvÄ› binÃ¡rnÃ­ch klasifikaÄnÃ­ch problÃ©mÅ¯.  
     $$\sigma(z) = \frac{1}{1 + e^{-z}}$$

     <img src="img/10/sigmoid.png" width="500px" />
     
   - **ReLU (Rectified Linear Unit)**: Produkuje vÃ½stupy v rozmezÃ­ [0, âˆ), Äasto pouÅ¾Ã­vanÃ¡ pro skrytÃ© vrstvy v hlubokÃ½ch neuronovÃ½ch sÃ­tÃ­ch.  
     $$\sigma(z) = \max(0, z)$$

     <img src="img/10/relu.png" width="500px" />

   - **Tanh (HyperbolickÃ¡ tangenta)**: Produkuje vÃ½stupy v rozmezÃ­ (-1, 1), Äasto pouÅ¾Ã­vanÃ¡ pÅ™i normalizaci dat.  
     $$\sigma(z) = \tanh(z)$$

     <img src="img/10/tanh.png" width="500px" />


### 7.5. Struktura NN
NeuronovÃ© sÃ­tÄ› se sklÃ¡dajÃ­ z vrstev neuronÅ¯:

- **VstupnÃ­ vrstva**: PÅ™ijÃ­mÃ¡ vstupy.
- **SkrytÃ© vrstvy**: ZpracovÃ¡vajÃ­ vstupy a uÄÃ­ se identifikovat sloÅ¾itÃ© vzory. MÅ¯Å¾e mÃ­t rÅ¯znÃ© mnoÅ¾stvÃ­ neuronÅ¯ a vrstev.
- **VÃ½stupnÃ­ vrstva**: Produkuje koneÄnÃ© vÃ½stupy. PoÄet (vÄ›tÅ¡inou) odpovÃ­dÃ¡ poÄtu hledanÃ½ch tÅ™Ã­d.

<img src="img/10/neuralNetwork.png" width="500px" />

### 7.6. Proces uÄenÃ­

UÄenÃ­ v neuronovÃ½ch sÃ­tÃ­ch probÃ­hÃ¡ v nÄ›kolika krocÃ­ch:

1. **NaÄtenÃ­ dat**: PÅ™ijetÃ­ vstupnÃ­ch dat.  
2. **Forward propagation**: Data prochÃ¡zÃ­ sÃ­tÃ­ a kaÅ¾dÃ¡ vrstva vypoÄÃ­tÃ¡vÃ¡ vÃ½stup.  
3. **VÃ½poÄet ztrÃ¡ty (Loss Function)**: Loss funkce mÄ›Å™Ã­ rozdÃ­l mezi predikovanÃ½m a skuteÄnÃ½m vÃ½stupem. NÄ›kterÃ© bÄ›Å¾nÃ© loss funkce zahrnujÃ­:
   - **Mean Squared Error (MSE)**: PouÅ¾Ã­vÃ¡ se pro regresi.  
     $$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
   - **Cross-Entropy Loss**: PouÅ¾Ã­vÃ¡ se pro klasifikaci.  
     $$H(p, q) = - \sum_{i} p(i) \log q(i)$$
4. **Backward propagation**: VÃ½poÄet gradientu loss funkce a Ãºprava vah sÃ­tÄ›.  
5. **Aktualizace vah**: PomocÃ­ optimalizÃ¡toru se vÃ¡hy upravujÃ­, aby se minimalizovala loss funkce.

  <img src="img/10/learning.svg" width="500px" />

## 8. Typy NeuronovÃ½ch SÃ­tÃ­

### 8.1. KonvoluÄnÃ­ NeuronovÃ© SÃ­tÄ› (CNN)

KonvoluÄnÃ­ neuronovÃ© sÃ­tÄ› jsou specifickÃ© pro prÃ¡ci s obrazem a pouÅ¾Ã­vajÃ­ konvoluÄnÃ­ vrstvy, kterÃ© aplikujÃ­ filtry na vstupnÃ­ data. Mezi populÃ¡rnÃ­ architektury patÅ™Ã­:
- **LeNet**: JednoduchÃ¡ architektura s mÃ©nÄ› vrstvami.
- **VGG**: PouÅ¾Ã­vÃ¡ homogennÃ­ konfiguraci 3x3 konvoluÄnÃ­ch vrstev.
- **Inception (GoogleNet)**: VyuÅ¾Ã­vÃ¡ incepÄnÃ­ bloky pro aplikaci filtrÅ¯ rÅ¯znÃ½ch velikostÃ­.
- **ResNet**: Obsahuje reziduÃ¡lnÃ­ spojenÃ­, kterÃ© usnadÅˆujÃ­ trÃ©nink hlubokÃ½ch sÃ­tÃ­.
- **Wide ResNet**: ZvyÅ¡uje Å¡Ã­Å™ku sÃ­tÄ› pro lepÅ¡Ã­ vÃ½kon.

Zde lze vidÄ›t historii vÃ½voje s nÄ›kterÃ½mi co jsem zmÃ­nil highlightovanÃ½mi:

<img src="img/10/architektures.png" width="500px" />

[NejlepÅ¡Ã­ mega pecka video2](https://youtu.be/KuXjwB4LzSA?si=9dbypH0UPSvOdBlC&t=514)

### 8.2. GenerativnÃ­ AdversariÃ¡lnÃ­ SÃ­tÄ› (GAN)

GANy generujÃ­ data podobnÃ¡ trÃ©ninkovÃ½m datÅ¯m pomocÃ­ dvou sÃ­tÃ­:
- **GenerÃ¡tor**: VytvÃ¡Å™Ã­ data.
- **DiskriminÃ¡tor**: HodnotÃ­, zda jsou data pravÃ¡ nebo generovanÃ¡.

AdversÃ¡rnÃ­ proces mezi tÄ›mito dvÄ›ma sÃ­tÄ›mi vede k rychlÃ©mu vylepÅ¡enÃ­ a vysokÃ© kvalitÄ› generovanÃ½ch vÃ½stupÅ¯. GenerÃ¡tor se snaÅ¾Ã­ pÅ™echytraÄit diskriminÃ¡tor a generovat lepÅ¡Ã­ a lepÅ¡Ã­ data a diskriminÃ¡tor zase hodnotit lÃ©pe a lÃ©pe (oba se vÃ½slednÄ› zlepÅ¡ujÃ­).

<img src="img/10/GAN.svg" width="500px" />

### 8.3. SamoorganizujÃ­cÃ­ se Mapy (Self-Organizing Maps, SOM)

SamoorganizujÃ­cÃ­ se mapy jsou typem nesupervizovanÃ© neuronovÃ© sÃ­tÄ›, kterÃ¡ se pouÅ¾Ã­vÃ¡ k redukci dimenzionality a vizualizaci dat. KaÅ¾dÃ½ neuron v mÅ™Ã­Å¾ce SOM odpovÃ­dÃ¡ urÄitÃ©mu segmentu datovÃ©ho prostoru a mapuje vstupnÃ­ data na 2D nebo 3D prostor.

- **PouÅ¾itÃ­**: 
  - Vizualizace vysokodimenzionÃ¡lnÃ­ch dat.
  - SeskupovÃ¡nÃ­ podobnÃ½ch datovÃ½ch bodÅ¯.

Model s 2D prostorem:

<img src="img/10/SOM.png" width="500px" />

### 8.4. Hopfieldovy SÃ­tÄ› (Hopfield Networks)

Hopfieldovy sÃ­tÄ› jsou typem rekurentnÃ­ neuronovÃ© sÃ­tÄ›. V tÃ©to sÃ­ti jsou vÅ¡echny neurony propojeny se vÅ¡emi ostatnÃ­mi (full-mesh) a pracujÃ­ binÃ¡rnÄ›. SÃ­Å¥ se trÃ©nuje na vzory, kterÃ© si mÃ¡ pamatovat, a pÅ™i aktivaci se snaÅ¾Ã­ najÃ­t nejbliÅ¾Å¡Ã­ uloÅ¾enÃ½ vzor.

- **PouÅ¾itÃ­**: 
  - AsociaÄnÃ­ pamÄ›Å¥.
  - OptimalizaÄnÃ­ problÃ©my.

<img src="img/10/hopfield.webp" width="500px" />

## 9. HodnocenÃ­ vÃ½konu

ExistujÃ­ rÅ¯znÃ© metriky hodnocenÃ­ vÃ½konu

[Video](https://www.youtube.com/watch?v=jJ7ff7Gcq34)

### 9.1. PÅ™esnost (Accuracy)

PÅ™esnost je nejÄastÄ›ji pouÅ¾Ã­vanou metrikou pro hodnocenÃ­ modelÅ¯ v klasifikaÄnÃ­ch problÃ©mech. Definuje se jako pomÄ›r sprÃ¡vnÄ› klasifikovanÃ½ch pÅ™Ã­padÅ¯ ku celkovÃ©mu poÄtu pozorovÃ¡nÃ­.  
$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

kde:
- TP (True Positives) jsou sprÃ¡vnÄ› identifikovanÃ© pozitivnÃ­ pÅ™Ã­pady.
- TN (True Negatives) jsou sprÃ¡vnÄ› identifikovanÃ© negativnÃ­ pÅ™Ã­pady.
- FP (False Positives) jsou chybnÄ› identifikovanÃ© pozitivnÃ­ pÅ™Ã­pady.
- FN (False Negatives) jsou chybnÄ› identifikovanÃ© negativnÃ­ pÅ™Ã­pady.

PÅ™esnost mÅ¯Å¾e bÃ½t nevhodnÃ¡ v situacÃ­ch, kdy jsou tÅ™Ã­dy cÃ­lovÃ© promÄ›nnÃ© nevyvÃ¡Å¾enÃ©.

### 9.2. Preciznost (Precision)

Preciznost je pomÄ›r sprÃ¡vnÄ› pozitivnÃ­ch pÅ™edpovÄ›dÃ­ z celkovÃ©ho poÄtu pozitivnÃ­ch pÅ™edpovÄ›dÃ­, kterÃ© algoritmus uÄinil.  
$$
\text{Precision} = \frac{TP}{TP + FP}
$$

### 9.3. Ãšplnost (Recall)

Ãšplnost pÅ™edstavuje, jakÃ½ poÄet relevantnÃ­ch datovÃ½ch poloÅ¾ek je sprÃ¡vnÄ› identifikovÃ¡n jako pozitivnÃ­. JinÃ½mi slovy, z datovÃ½ch poloÅ¾ek, kterÃ© jsou ve skuteÄnosti pozitivnÃ­, kolik z nich bylo sprÃ¡vnÄ› identifikovÃ¡no jako pozitivnÃ­ algoritmem.  
$$
\text{Recall} = \frac{TP}{TP + FN}
$$

### 9.4. F1 SkÃ³re (F1-Score)

F1 skÃ³re je harmonickÃ½ prÅ¯mÄ›r preciznosti a Ãºplnosti, kterÃ½ bere do Ãºvahy obÄ› tyto metriky pro vyhodnocenÃ­ vÃ½konu algoritmu.  
$$
\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

## 10. Algoritmy a strategie uÄenÃ­

### 10.1. GradientnÃ­ sestup a jeho varianty

[Video co je to gradient](https://www.youtube.com/watch?v=tIpKfDc295M)

[Video co je to gradientnÃ­ sestup](https://www.youtube.com/watch?v=qg4PchTECck)

GradientnÃ­ sestup (Gradient Descent) je zÃ¡kladnÃ­ optimalizaÄnÃ­ algoritmus pouÅ¾Ã­vanÃ½ pro trÃ©nink neuronovÃ½ch sÃ­tÃ­. CÃ­lem gradientnÃ­ho sestupu je minimalizovat chybu modelu iterativnÃ­m upravovÃ¡nÃ­m vah smÄ›rem k minimÃ¡lnÃ­ hodnotÄ› chyby. GradientnÃ­ vektor je smÄ›r nejvÄ›tÅ¡Ã­ zmÄ›ny vypoÄÃ­tanÃ½ z parciÃ¡lnÃ­ch derivacÃ­ funkce. TudÃ­Å¾ u uÄenÃ­ je gradientnÃ­ sestup sestupovÃ¡nÃ­ smÄ›rem kterÃ½ urÄuje gradientnÃ­ vektor smÄ›rem k minimu. 

GradientnÃ­ sestup v prostoru
<img src="img/10/grad_desc.gif" width="500px" />

- **Stochastic Gradient Descent (SGD)**: Aktualizuje vÃ¡hy na zÃ¡kladÄ› jednoho trÃ©ninkovÃ©ho vzorku v kaÅ¾dÃ© iteraci, coÅ¾ mÅ¯Å¾e urychlit konvergenci, ale zÃ¡roveÅˆ mÅ¯Å¾e zpÅ¯sobit vÄ›tÅ¡Ã­ oscilace (menÅ¡Ã­ stabilitu).
- **Mini-Batch Gradient Descent**: Aktualizuje vÃ¡hy na zÃ¡kladÄ› malÃ© nÃ¡hodnÃ© podmnoÅ¾iny dat (mini-batch), ÄÃ­mÅ¾ se dosahuje lepÅ¡Ã­ stability a rychlosti neÅ¾ u SGD.
- **Adam (Adaptive Moment Estimation)**: PokroÄilÃ½ optimalizÃ¡tor, kterÃ½ dynamicky upravuje uÄÃ­cÃ­ rychlost pro kaÅ¾dou vÃ¡hu pomocÃ­ prvnÃ­ch a druhÃ½ch momentÅ¯ gradientÅ¯. Je v dneÅ¡nÃ­ dobÄ› naprosto DOMINUJÃCÃ jako optimalizÃ¡tor. Existuje na nÄ›j samostatnÃ¡ prÃ¡ce na XX strÃ¡nek takÅ¾e detaily jsou troÅ¡ku out of scope na bakalÃ¡Å™ovi. 

### 10.2. Backpropagation

Backpropagation je klÃ­ÄovÃ½ algoritmus pro trÃ©nink vÃ­cevrstvÃ½ch neuronovÃ½ch sÃ­tÃ­. Tento algoritmus poÄÃ­tÃ¡ gradient chyby s ohledem na vÃ¡hy sÃ­tÄ› a pouÅ¾Ã­vÃ¡ gradientnÃ­ sestup k aktualizaci tÄ›chto vah.

- **Forward pass**: Data prochÃ¡zejÃ­ sÃ­tÃ­ od vstupu k vÃ½stupu a vypoÄÃ­tÃ¡ se predikce.
- **Backward pass**: VypoÄÃ­tÃ¡ se chyba mezi predikcÃ­ a skuteÄnÃ½m vÃ½stupem, potÃ© se chyba Å¡Ã­Å™Ã­ zpÄ›t sÃ­tÃ­, aby se vypoÄÃ­taly gradienty.
- **Aktualizace vah**: VÃ¡hy se upravÃ­ pomocÃ­ gradientnÃ­ho sestupu podle vypoÄÃ­tanÃ½ch gradientÅ¯.


### 10.3. Metody regularizace

Regularizace je technika pouÅ¾Ã­vanÃ¡ k prevenci pÅ™etrÃ©novÃ¡nÃ­ modelu tÃ­m, Å¾e penalizuje sloÅ¾itost modelu. Regularizace pomÃ¡hÃ¡ modelu generalizovat lÃ©pe na neznÃ¡mÃ¡ data. (BrÃ¡nÃ­ overfittingu)

[Video](https://www.youtube.com/watch?v=CgbbvozFgXo)

- **L1 Regularizace (Lasso)**: PÅ™idÃ¡vÃ¡ penalizaci absolutnÃ­ hodnoty vah do loss funkce. MÃ¡ tendenci vytvÃ¡Å™et Å™Ã­dkÃ© modely, kde nÄ›kterÃ© vÃ¡hy mohou bÃ½t nulovÃ©.
  \[
  L1\_penalty = \lambda \sum_{i} |w_i|
  \]
  PÅ™Ã­klad: V lineÃ¡rnÃ­ regresi, L1 regularizace mÅ¯Å¾e vÃ©st k modelÅ¯m, kde nÄ›kterÃ© koeficienty jsou pÅ™esnÄ› nulovÃ©, coÅ¾ zjednoduÅ¡uje model a umoÅ¾Åˆuje identifikaci nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ch promÄ›nnÃ½ch.

- **L2 Regularizace (Ridge)**: PÅ™idÃ¡vÃ¡ penalizaci druhÃ© mocniny vah do loss funkce. PomÃ¡hÃ¡ sniÅ¾ovat velikost vah a zabraÅˆuje pÅ™Ã­liÅ¡ vysokÃ½m hodnotÃ¡m.
  \[
  L2\_penalty = \lambda \sum_{i} w_i^2
  \]
  PÅ™Ã­klad: V lineÃ¡rnÃ­ regresi, L2 regularizace rozprostÃ­rÃ¡ vÃ¡hy vÃ­ce rovnomÄ›rnÄ›, coÅ¾ mÅ¯Å¾e zlepÅ¡it predikÄnÃ­ vÃ½kon na novÃ½ch datech.

- **Dropout**: BÄ›hem trÃ©ninku nÃ¡hodnÄ› vypÃ­nÃ¡ urÄitÃ© procento neuronÅ¯ v sÃ­ti, coÅ¾ zabraÅˆuje vzniku zÃ¡vislostÃ­ mezi neurony a zlepÅ¡uje generalizaci modelu.
  \[
  y = f(W \cdot (x \odot r) + b)
  \]
  kde \( r \) je maska dropoutu, kterÃ¡ nÃ¡hodnÄ› nastavuje nÄ›kterÃ© vstupy na nulu.

- **Early Stopping**: Sleduje vÃ½kon modelu na validaÄnÃ­ sadÄ› a zastavuje trÃ©nink, jakmile se vÃ½kon zaÄne zhorÅ¡ovat, aby se zabrÃ¡nilo pÅ™etrÃ©novÃ¡nÃ­.
  PÅ™Ã­klad: Pokud pÅ™esnost modelu na validaÄnÃ­ sadÄ› pÅ™estane rÅ¯st a zaÄne klesat, trÃ©nink se zastavÃ­, ÄÃ­mÅ¾ se minimalizuje riziko pÅ™etrÃ©novÃ¡nÃ­.


## 11. KÅ™Ã­Å¾ovÃ¡ validace (Cross-Validation)

JednÃ¡ se o zpÅ¯sob, jak upravit uÄenÃ­ dÄ›lenÃ­m do rÅ¯znÃ½ch podmnoÅ¾in dat a snÃ­Å¾enÃ­ problÃ©mÅ¯ spojenÃ½ch s pÅ™etrÃ©novÃ¡nÃ­m.

[Video](https://www.youtube.com/watch?v=fSytzGwwBVw)

### 11.1. K-fold Cross-Validation

Jedna z nejbÄ›Å¾nÄ›jÅ¡Ã­ch metod kÅ™Ã­Å¾ovÃ© validace je K-fold cross-validation, kterÃ¡ rozdÄ›lÃ­ dataset na K stejnÃ½ch ÄÃ¡stÃ­ (foldÅ¯). Model je postupnÄ› trÃ©novÃ¡n na K-1 ÄÃ¡stech a testovÃ¡n na zbÃ½vajÃ­cÃ­ ÄÃ¡sti. Tento proces se opakuje K-krÃ¡t, pÅ™iÄemÅ¾ kaÅ¾dÃ¡ ÄÃ¡st je pouÅ¾ita jednou jako testovacÃ­ sada.

<img src="img/10/kcross.png" width="500px" />

- **Proces**:
  1. RozdÄ›lenÃ­ dat na K ÄÃ¡stÃ­.
  2. TrÃ©nink modelu na K-1 ÄÃ¡stech a testovÃ¡nÃ­ na jednÃ© ÄÃ¡sti.
  3. OpakovÃ¡nÃ­ procesu K-krÃ¡t.
  4. PrÅ¯mÄ›rovÃ¡nÃ­ vÃ½sledkÅ¯ z K iteracÃ­ pro zÃ­skÃ¡nÃ­ koneÄnÃ©ho hodnocenÃ­ modelu.

- **VÃ½hody**:
  - LepÅ¡Ã­ vyuÅ¾itÃ­ dat pro trÃ©nink a testovÃ¡nÃ­.
  - SnÃ­Å¾enÃ­ rizika pÅ™etrÃ©novÃ¡nÃ­ a zÃ­skÃ¡nÃ­ robustnÄ›jÅ¡Ã­ho odhadu vÃ½konu modelu.

- **NevÃ½hody**:
  - VyÅ¡Å¡Ã­ vÃ½poÄetnÃ­ nÃ¡roÄnost kvÅ¯li opakovanÃ©mu trÃ©novÃ¡nÃ­ modelu.

### 11.2. Leave-One-Out Cross-Validation (LOOCV)

LOOCV je speciÃ¡lnÃ­ pÅ™Ã­pad K-fold cross-validation, kde K je rovno poÄtu vzorkÅ¯ v datasetu. KaÅ¾dÃ½ vzorek je pouÅ¾it jednou jako testovacÃ­ sada a zbytek jako trÃ©ninkovÃ¡ sada.

- **Proces**:
  1. Pro kaÅ¾dÃ½ vzorek v datasetu: 
     - PouÅ¾itÃ­ vzorku jako testovacÃ­ sady.
     - PouÅ¾itÃ­ zbytku dat jako trÃ©ninkovÃ© sady.
  2. OpakovÃ¡nÃ­ procesu pro kaÅ¾dÃ½ vzorek.

- **VÃ½hody**:
  - MaximÃ¡lnÃ­ vyuÅ¾itÃ­ dat pro trÃ©nink.
  - SnÃ­Å¾enÃ­ moÅ¾nosti pÅ™etrÃ©novÃ¡nÃ­.

- **NevÃ½hody**:
  - VysokÃ¡ vÃ½poÄetnÃ­ nÃ¡roÄnost zejmÃ©na u velkÃ½ch datasetÅ¯.



